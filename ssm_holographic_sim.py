"""
SSM Holographic Vacuum Simulation
===================================
Constructive verification of K=12 lattice saturation via
2D-dominant holographic growth with proximity bonding.

Usage:
  python ssm_holographic_sim.py [--nodes N] [--lift PROB] [--sweep]
  python ssm_holographic_sim.py --analyze          # Full Section 3 analysis
  python ssm_holographic_sim.py --rex-sweep         # Exclusion radius sweep

Author: Raghu Kulkarni (raghu@idrive.com)
License: MIT
"""

import numpy as np
import networkx as nx
from scipy.spatial import cKDTree
from collections import Counter
import random
import argparse
import time

# ==============================================================
# Physical Constants
# ==============================================================
UNIT_LENGTH = 1.0           # Stitch length L (Planck length)
HARD_SHELL = 0.95           # Exclusion radius (accommodates
                            # Regge deficit angle delta ~0.128 rad)
BOND_RADIUS = 1.05          # Proximity bonding threshold
LIFT_HEIGHT = np.sqrt(2.0/3.0)   # Tetrahedral apex: h = sqrt(2/3) * L
LATERAL_HEIGHT = np.sqrt(3.0)/2   # In-plane equilateral: h = sqrt(3)/2 * L

# ==============================================================
# Core Simulation
# ==============================================================
class SSMHolographicSim:
    """
    Builds a discrete vacuum via two operators:

    Stitch (2D): Laterally extends a hexagonal sheet (K=6 in-plane).
    Lift  (3D): Projects a node from a triangular face at height
                h = sqrt(2/3) * L, seeding a new adjacent layer.

    Proximity Bonding: After every batch of new nodes, all pairs
    within distance BOND_RADIUS automatically form edges.
    This is the entanglement mechanism that connects adjacent 2D layers,
    producing the 6(in-plane) + 3(above) + 3(below) = K=12
    Cuboctahedral coordination of the FCC lattice.

    Note: The theoretical lift probability is p = e^{-3} ~ 0.04879 [Coleman 1977].
    We use p = 0.05 as a computational proxy; the 0.4% difference produces
    <2% variation in surface shell thickness and <5% in coordination statistics.
    """

    def __init__(self, target_nodes=5000, lift_prob=0.05):
        self.target = target_nodes
        self.lift_prob = lift_prob

        self.nodes = []             # List of np.array positions
        self.G = nx.Graph()         # Topological graph
        self.triangles = set()      # All detected triangles
        self.active_triangles = set()  # Triangles available for Lift
        self.active_edges = set()   # Edges available for Stitch
        self.buffer = []            # New nodes pending tree rebuild

    def _add_node(self, pos):
        idx = len(self.nodes)
        p = np.array(pos, dtype=float)
        self.nodes.append(p)
        self.buffer.append(p)
        self.G.add_node(idx)
        return idx

    def _add_edge(self, u, v):
        if self.G.has_edge(u, v) or u == v:
            return
        self.G.add_edge(u, v)
        self.active_edges.add((min(u, v), max(u, v)))
        for w in set(self.G.neighbors(u)) & set(self.G.neighbors(v)):
            tri = tuple(sorted((u, v, w)))
            if tri not in self.triangles:
                self.triangles.add(tri)
                self.active_triangles.add(tri)

    def _proximity_stitch(self, tree):
        for u, v in tree.query_pairs(r=BOND_RADIUS):
            if not self.G.has_edge(u, v):
                self._add_edge(u, v)

    def _is_valid(self, candidate, tree):
        if tree and tree.query_ball_point(candidate, HARD_SHELL):
            return False
        for p in self.buffer:
            if np.linalg.norm(candidate - p) < HARD_SHELL:
                return False
        return True

    def _stitch_lateral(self, edge, tree):
        u, v = edge
        p1, p2 = self.nodes[u], self.nodes[v]
        mid = (p1 + p2) / 2.0

        attached = [t for t in self.active_triangles
                    if u in t and v in t]
        if attached:
            w = [n for n in attached[0] if n != u and n != v][0]
            direction = mid - self.nodes[w]
        else:
            direction = np.cross(p2 - p1, [0, 0, 1])

        norm = np.linalg.norm(direction)
        if norm > 0:
            direction /= norm
        else:
            direction = np.array([0.0, 1.0, 0.0])

        c1 = mid + direction * LATERAL_HEIGHT * UNIT_LENGTH
        c2 = mid - direction * LATERAL_HEIGHT * UNIT_LENGTH
        v1, v2 = self._is_valid(c1, tree), self._is_valid(c2, tree)

        if not v1 and not v2:
            self.active_edges.discard((min(u, v), max(u, v)))
            return False

        cand = c1 if v1 else c2
        nid = self._add_node(cand)
        self._add_edge(nid, u)
        self._add_edge(nid, v)
        return True

    def _lift_triangle(self, tri, tree):
        u, v, w = tri
        p0, p1, p2 = self.nodes[u], self.nodes[v], self.nodes[w]
        centroid = (p0 + p1 + p2) / 3.0
        normal = np.cross(p1 - p0, p2 - p0)
        norm = np.linalg.norm(normal)

        if norm == 0:
            self.active_triangles.discard(tri)
            return False
        normal /= norm

        signs = [1, -1] if random.random() > 0.5 else [-1, 1]
        for s in signs:
            cand = centroid + normal * LIFT_HEIGHT * UNIT_LENGTH * s
            if self._is_valid(cand, tree):
                nid = self._add_node(cand)
                self._add_edge(nid, u)
                self._add_edge(nid, v)
                self._add_edge(nid, w)
                return True

        self.active_triangles.discard(tri)
        return False

    def run(self, verbose=True):
        t0 = time.time()

        self._add_node([0.0, 0.0, 0.0])
        self._add_node([UNIT_LENGTH, 0.0, 0.0])
        self._add_node([0.5 * UNIT_LENGTH, LATERAL_HEIGHT * UNIT_LENGTH, 0.0])
        self._add_edge(0, 1)
        self._add_edge(1, 2)
        self._add_edge(2, 0)

        tree = cKDTree(self.nodes)
        self.buffer = []
        attempts = 0

        while len(self.nodes) < self.target and attempts < 500000:
            if len(self.buffer) > 40:
                tree = cKDTree(self.nodes)
                self.buffer = []
                self._proximity_stitch(tree)

            can_lift = len(self.active_triangles) > 0
            can_stitch = len(self.active_edges) > 0

            if random.random() < self.lift_prob and can_lift:
                tri = random.choice(sorted(self.active_triangles))
                ok = self._lift_triangle(tri, tree)
            elif can_stitch:
                edge = random.choice(sorted(self.active_edges))
                ok = self._stitch_lateral(edge, tree)
            else:
                ok = False

            attempts = 0 if ok else attempts + 1

            if verbose and len(self.nodes) % 1000 == 0 and ok:
                stats = self.get_stats()
                elapsed = time.time() - t0
                print(f"N={stats['n']:5d} | K_mean={stats['k_mean']:.2f} "
                      f"| K=12: {stats['pct_k12']:.1f}% "
                      f"| K>=10: {stats['pct_k10']:.1f}% | {elapsed:.1f}s")

        tree = cKDTree(self.nodes)
        self._proximity_stitch(tree)

        elapsed = time.time() - t0
        if verbose:
            status = "COMPLETE" if len(self.nodes) >= self.target else "JAMMED"
            print(f"--- {status}: {len(self.nodes)} nodes in {elapsed:.1f}s ---")

        return self

    def get_stats(self):
        degrees = np.array([d for _, d in self.G.degree()])
        dd = Counter(degrees)
        n = len(self.nodes)
        k12 = dd.get(12, 0)
        k10 = sum(dd[k] for k in dd if k >= 10)
        return {
            'n': n,
            'k_max': int(degrees.max()) if n > 0 else 0,
            'k_mean': float(degrees.mean()) if n > 0 else 0,
            'k12': k12,
            'k10': k10,
            'pct_k12': 100.0 * k12 / n if n > 0 else 0,
            'pct_k10': 100.0 * k10 / n if n > 0 else 0,
            'deg_dist': dict(sorted(dd.items())),
            'edges': self.G.number_of_edges(),
            'triangles': len(self.triangles),
        }

    def count_emergent_squares(self):
        squares = 0
        checked = set()
        for u in self.G.nodes():
            u_nbrs = list(self.G.neighbors(u))
            for i, a in enumerate(u_nbrs):
                for b in u_nbrs[i+1:]:
                    if self.G.has_edge(a, b):
                        continue
                    key = (min(a, b), max(a, b))
                    if key in checked:
                        continue
                    checked.add(key)
                    for c in set(self.G.neighbors(a)) & set(self.G.neighbors(b)) - {u}:
                        if self.G.has_edge(c, u):
                            continue
                        pu, pa, pb, pc = (self.nodes[u], self.nodes[a],
                                          self.nodes[b], self.nodes[c])
                        d1 = np.linalg.norm(pu - pc)
                        d2 = np.linalg.norm(pa - pb)
                        if 1.2 < d1 < 1.6 and 1.2 < d2 < 1.6:
                            squares += 1
        return squares

    def analyze_layers(self, tol=0.2):
        """
        Post-processing analysis of emergent layer structure.
        Identifies planar layers via z-clustering, then measures:
          - Layer flatness (sigma_z per layer)
          - Inter-layer spacing vs ideal FCC value sqrt(2/3)
          - Inter-layer bond statistics
          - Surface shell thickness delta

        Used to generate results for Sections 3.6-3.7.
        """
        positions = np.array(self.nodes)
        degrees = np.array([d for _, d in self.G.degree()])
        z_vals = positions[:, 2]

        # --- Layer identification via z-clustering ---
        visited = np.zeros(len(positions), dtype=bool)
        z_order = np.argsort(z_vals)
        layers = []
        layer_ids = np.full(len(positions), -1, dtype=int)
        idx = 0
        for i in z_order:
            if visited[i]:
                continue
            z_c = z_vals[i]
            mask = (np.abs(z_vals - z_c) < tol) & (~visited)
            members = np.where(mask)[0]
            if len(members) >= 3:
                layers.append(members)
                layer_ids[members] = idx
                visited[members] = True
                idx += 1
            else:
                visited[members] = True

        # --- Layer flatness ---
        substantial = [(i, m) for i, m in enumerate(layers) if len(m) >= 10]
        sigmas = []
        for i, m in substantial:
            sigmas.append({
                'layer': i,
                'n_nodes': len(m),
                'sigma_z': float(np.std(positions[m, 2])),
                'z_mean': float(np.mean(positions[m, 2]))
            })
        n_perfect = sum(1 for s in sigmas if s['sigma_z'] < 1e-10)

        # --- Inter-layer spacing ---
        z_means = sorted([s['z_mean'] for s in sigmas])
        spacings = np.diff(z_means) if len(z_means) > 1 else np.array([])
        ideal = np.sqrt(2.0 / 3.0)

        # --- Inter-layer bonds ---
        in_layer = layer_ids >= 0
        inter_bonds = np.zeros(len(positions), dtype=int)
        for node in range(len(positions)):
            if layer_ids[node] < 0:
                continue
            for nbr in self.G.neighbors(node):
                if layer_ids[nbr] >= 0 and layer_ids[nbr] != layer_ids[node]:
                    inter_bonds[node] += 1
        il_counts = inter_bonds[in_layer]

        # --- Surface shell thickness ---
        centroid = positions.mean(axis=0)
        radii = np.linalg.norm(positions - centroid, axis=1)
        R_max = float(radii.max())
        bulk_mask = degrees == 12
        R_bulk = float(radii[bulk_mask].max()) if np.any(bulk_mask) else 0.0
        delta = R_max - R_bulk

        return {
            'n_layers': len(layers),
            'nodes_in_layers': int(sum(len(m) for m in layers)),
            'n_substantial': len(substantial),
            'n_perfect_flat': n_perfect,
            'layer_details': sigmas,
            'spacings': spacings.tolist(),
            'spacing_mean': float(spacings.mean()) if len(spacings) > 0 else 0.0,
            'spacing_std': float(spacings.std()) if len(spacings) > 0 else 0.0,
            'ideal_spacing': ideal,
            'il_bonds_mean': float(il_counts.mean()) if len(il_counts) > 0 else 0.0,
            'il_frac_bonded': float(np.mean(il_counts > 0)) if len(il_counts) > 0 else 0.0,
            'R_max': R_max,
            'R_bulk': R_bulk,
            'delta': delta,
        }


def main():
    parser = argparse.ArgumentParser(
        description="SSM Holographic Vacuum Simulation")
    parser.add_argument('--nodes', type=int, default=5000)
    parser.add_argument('--lift', type=float, default=0.05)
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--sweep', action='store_true')
    parser.add_argument('--rex-sweep', action='store_true')
    parser.add_argument('--analyze', action='store_true')
    args = parser.parse_args()

    random.seed(args.seed)
    np.random.seed(args.seed)

    if args.sweep:
        print("=" * 65)
        print("TABLE 1: Lift Probability Sweep (N=3000)")
        print("=" * 65)
        print(f"{'Lift%':>6} | {'K_mean':>7} | {'%K=12':>7} | {'%K>=10':>7}")
        print("-" * 45)
        for lp in [0.05, 0.10, 0.15, 0.20, 0.30, 0.50, 0.85]:
            random.seed(args.seed); np.random.seed(args.seed)
            sim = SSMHolographicSim(target_nodes=3000, lift_prob=lp)
            sim.run(verbose=False)
            s = sim.get_stats()
            print(f"{lp*100:5.0f}% | {s['k_mean']:7.2f} | "
                  f"{s['pct_k12']:6.1f}% | {s['pct_k10']:6.1f}%")

    elif args.rex_sweep:
        print("=" * 65)
        print("EXCLUSION RADIUS SENSITIVITY (N=500, Lift=5%)")
        print("=" * 65)
        print(f"{'Rex':>6} | {'Nodes':>6} | {'K_max':>6} | {'K_mean':>7}")
        print("-" * 40)
        import copy
        for rex in np.arange(0.50, 1.02, 0.02):
            random.seed(args.seed); np.random.seed(args.seed)
            global HARD_SHELL
            old_hs = HARD_SHELL
            HARD_SHELL = rex
            sim = SSMHolographicSim(target_nodes=500, lift_prob=0.05)
            sim.run(verbose=False)
            s = sim.get_stats()
            jammed = "JAMMED" if s['n'] < 450 else ""
            print(f"{rex:5.2f} | {s['n']:6d} | {s['k_max']:6d} | "
                  f"{s['k_mean']:7.2f} {jammed}")
            HARD_SHELL = old_hs

    elif args.analyze:
        print("=" * 65)
        print(f"SSM FULL ANALYSIS â€” Sections 3.6 & 3.7")
        print(f"Lift={args.lift*100:.0f}%, Seed={args.seed}")
        print("=" * 65)

        # ---- Section 3.6: Finite-Size Scaling ----
        print("\n" + "=" * 50)
        print("TABLE 2: Finite-Size Scaling (Section 3.6)")
        print("=" * 50)
        print(f"{'N':>6}  {'K_mean':>7}  {'K=12%':>7}  {'K>=10%':>7}  {'Edges':>7}")
        print("-" * 45)
        scale_data = []
        for N in [500, 1000, 2000, 3000, 5000]:
            random.seed(args.seed); np.random.seed(args.seed)
            sim = SSMHolographicSim(target_nodes=N, lift_prob=args.lift)
            sim.run(verbose=False)
            s = sim.get_stats()
            print(f"{s['n']:6d}  {s['k_mean']:7.2f}  {s['pct_k12']:6.1f}%  "
                  f"{s['pct_k10']:6.1f}%  {s['edges']:7d}")
            scale_data.append((s['n'], s['pct_k12'] / 100.0))

        Ns = np.array([d[0] for d in scale_data])
        fs = np.array([d[1] for d in scale_data])
        # Asymptotic fit from largest system (most reliable for N^{-1/3} scaling)
        alpha_asym = float((1 - fs[-1]) * Ns[-1]**(1.0/3.0))
        # OLS fit across all sizes
        x = Ns**(-1.0/3.0)
        y = 1.0 - fs
        alpha_ols = float(np.sum(x * y) / np.sum(x * x))
        alpha = alpha_asym  # Use asymptotic (conservative, matches paper)
        print(f"\nScaling law: f(K=12) = 1 - alpha / N^(1/3)")
        print(f"  alpha (N=5000 asymptotic): {alpha_asym:.1f}")
        print(f"  alpha (OLS all points):    {alpha_ols:.1f}")
        print(f"  Using alpha = {alpha:.1f} (asymptotic, conservative)")
        print("Extrapolations:")
        for Nex, label in [(1e4, '10^4'), (1e6, '10^6'),
                           (1e9, '10^9'), (1e60, '10^60 (universe)')]:
            fex = max(0, 1 - alpha / Nex**(1.0/3.0))
            print(f"  N = {label:>20s}:  K=12 = {fex*100:.4f}%")

        # ---- Section 3.7: Layer Analysis ----
        print("\n" + "=" * 50)
        print(f"LAYER ANALYSIS (N={args.nodes}, Section 3.7)")
        print("=" * 50)
        random.seed(args.seed); np.random.seed(args.seed)
        sim = SSMHolographicSim(target_nodes=args.nodes, lift_prob=args.lift)
        sim.run(verbose=False)
        s = sim.get_stats()
        la = sim.analyze_layers()

        print(f"\nBasic statistics:")
        print(f"  Nodes:          {s['n']}")
        print(f"  Edges:          {s['edges']}")
        print(f"  Triangles:      {s['triangles']}")
        print(f"  K_max:          {s['k_max']}")
        print(f"  K_mean:         {s['k_mean']:.2f}")
        print(f"  K=12:           {s['k12']} ({s['pct_k12']:.1f}%)")
        print(f"  K>=10:          {s['k10']} ({s['pct_k10']:.1f}%)")

        print(f"\nLayer structure:")
        print(f"  Layers:         {la['n_layers']}")
        print(f"  Nodes in layers:{la['nodes_in_layers']} / {s['n']} "
              f"({100*la['nodes_in_layers']/s['n']:.1f}%)")
        print(f"  Substantial:    {la['n_substantial']} (>=10 nodes)")
        print(f"  Perfectly flat:  {la['n_perfect_flat']} / {la['n_substantial']} "
              f"(sigma_z < 1e-10)")

        print(f"\nInter-layer spacing:")
        print(f"  Measured:       {la['spacing_mean']:.4f} +/- {la['spacing_std']:.4f} L")
        print(f"  Ideal FCC:      {la['ideal_spacing']:.4f} L  [sqrt(2/3)]")
        err = abs(la['spacing_mean'] - la['ideal_spacing']) / la['ideal_spacing'] * 100
        print(f"  Error:          {err:.2f}%")

        print(f"\nInter-layer bonding:")
        print(f"  Mean IL bonds:  {la['il_bonds_mean']:.1f} per node")
        print(f"  Nodes bonded:   {la['il_frac_bonded']*100:.1f}%")

        print(f"\nSurface shell:")
        print(f"  R_max:          {la['R_max']:.2f} L")
        print(f"  R_bulk (K=12):  {la['R_bulk']:.2f} L")
        print(f"  delta:          {la['delta']:.2f} L_Planck")

        nsq = sim.count_emergent_squares()
        print(f"\nEmergent squares: {nsq}")
        if nsq > 0:
            print(f"Tri:Sq ratio = {s['triangles']/nsq:.1f}:1 (cuboctahedron: 1.33:1)")

    else:
        print(f"SSM Holographic Simulation")
        print(f"Nodes: {args.nodes}, Lift: {args.lift*100:.0f}%, Seed: {args.seed}")
        print(f"Rex={HARD_SHELL}, Bond={BOND_RADIUS}\n")

        sim = SSMHolographicSim(target_nodes=args.nodes, lift_prob=args.lift)
        sim.run(verbose=True)
        s = sim.get_stats()

        print(f"\n{'='*50}\nFINAL RESULTS\n{'='*50}")
        print(f"Nodes: {s['n']}\nEdges: {s['edges']}\nTriangles: {s['triangles']}")
        print(f"K_max: {s['k_max']}\nK_mean: {s['k_mean']:.2f}")
        print(f"K=12: {s['k12']} ({s['pct_k12']:.1f}%)\nK>=10: {s['k10']} ({s['pct_k10']:.1f}%)\n")

        if s['n'] <= 5000:
            print(f"Counting emergent square faces...")
            nsq = sim.count_emergent_squares()
            print(f"Emergent squares: {nsq}")
            if nsq > 0:
                print(f"Tri:Sq ratio = {s['triangles']/nsq:.1f}:1 (cuboctahedron: 1.33:1)")


if __name__ == "__main__":
    main()
